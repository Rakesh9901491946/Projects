{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to 'latest_stack_overflow_data.csv'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Your API key\n",
    "# Replace with your actual API key from Stack Exchange\n",
    "api_key = import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Your Stack Exchange API key\n",
    "api_key = \"YOUR_STACK_EXCHANGE_API_KEY\"  # Replace with your actual API key\n",
    "\n",
    "# Define the number of records and pagesize\n",
    "total_records = 10000\n",
    "pagesize = 100  # Max allowed pagesize by the API\n",
    "\n",
    "def fetch_user_data():\n",
    "    all_data = []\n",
    "    page = 1\n",
    "    while len(all_data) < total_records:\n",
    "        url = (\n",
    "            f\"https://api.stackexchange.com/2.3/users\"\n",
    "            f\"?order=desc&sort=creation&site=stackoverflow\"\n",
    "            f\"&pagesize={pagesize}&page={page}&key={api_key}\"\n",
    "        )\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            results = response.json().get('items', [])\n",
    "            if not results:\n",
    "                # No more results\n",
    "                break\n",
    "            for user in results:\n",
    "                data_row = {\n",
    "                    'account_id': user.get('account_id'),\n",
    "                    'reputation': user.get('reputation'),\n",
    "                    'user_id': user.get('user_id'),\n",
    "                    'user_type': user.get('user_type'),\n",
    "                    'profile_image': user.get('profile_image'),\n",
    "                    'display_name': user.get('display_name'),\n",
    "                    'link': user.get('link'),\n",
    "                    'creation_date': datetime.fromtimestamp(user.get('creation_date')),\n",
    "                    'last_access_date': datetime.fromtimestamp(user.get('last_access_date')),\n",
    "                }\n",
    "                all_data.append(data_row)\n",
    "                if len(all_data) >= total_records:\n",
    "                    break\n",
    "            page += 1\n",
    "        else:\n",
    "            print(f\"Error fetching data: {response.status_code}\")\n",
    "            break\n",
    "        # Add time.sleep to respect API rate limits\n",
    "        time.sleep(1.1)  # Sleep for 1.1 seconds\n",
    "    return all_data\n",
    "\n",
    "# Fetch data\n",
    "print(\"Fetching user data from Stack Exchange API...\")\n",
    "data = fetch_user_data()\n",
    "print(f\"Fetched {len(data)} records.\")\n",
    "\n",
    "# Create pandas DataFrame from data\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Add index column\n",
    "df.reset_index(inplace=True)\n",
    "df.rename(columns={'index': 'id'}, inplace=True)\n",
    "\n",
    "# Reorder columns if needed\n",
    "desired_columns = [\n",
    "    'id', 'account_id', 'reputation', 'user_id', 'user_type',\n",
    "    'profile_image', 'display_name', 'link', 'creation_date',\n",
    "    'last_access_date'\n",
    "]\n",
    "df = df[desired_columns]\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "output_file = 'latest_stack_overflow_user_data.csv'\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f'Data saved to {output_file}')\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Your Stack Exchange API key\n",
    "api_key = \"YOUR_STACK_EXCHANGE_API_KEY\"  # Replace with your actual API key\n",
    "\n",
    "# Define the number of records and pagesize\n",
    "total_records = 10000\n",
    "pagesize = 100  # Max allowed pagesize by the API\n",
    "\n",
    "def fetch_user_data():\n",
    "    all_data = []\n",
    "    page = 1\n",
    "    while len(all_data) < total_records:\n",
    "        url = (\n",
    "            f\"https://api.stackexchange.com/2.3/users\"\n",
    "            f\"?order=desc&sort=creation&site=stackoverflow\"\n",
    "            f\"&pagesize={pagesize}&page={page}&key={api_key}\"\n",
    "        )\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            results = response.json().get('items', [])\n",
    "            if not results:\n",
    "                # No more results\n",
    "                break\n",
    "            for user in results:\n",
    "                data_row = {\n",
    "                    'account_id': user.get('account_id'),\n",
    "                    'reputation': user.get('reputation'),\n",
    "                    'user_id': user.get('user_id'),\n",
    "                    'user_type': user.get('user_type'),\n",
    "                    'profile_image': user.get('profile_image'),\n",
    "                    'display_name': user.get('display_name'),\n",
    "                    'link': user.get('link'),\n",
    "                    'creation_date': datetime.fromtimestamp(user.get('creation_date')),\n",
    "                    'last_access_date': datetime.fromtimestamp(user.get('last_access_date')),\n",
    "                }\n",
    "                all_data.append(data_row)\n",
    "                if len(all_data) >= total_records:\n",
    "                    break\n",
    "            page += 1\n",
    "        else:\n",
    "            print(f\"Error fetching data: {response.status_code}\")\n",
    "            break\n",
    "        # Add time.sleep to respect API rate limits\n",
    "        time.sleep(1.1)  # Sleep for 1.1 seconds\n",
    "    return all_data\n",
    "\n",
    "# Fetch data\n",
    "print(\"Fetching user data from Stack Exchange API...\")\n",
    "data = fetch_user_data()\n",
    "print(f\"Fetched {len(data)} records.\")\n",
    "\n",
    "# Create pandas DataFrame from data\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Add index column\n",
    "df.reset_index(inplace=True)\n",
    "df.rename(columns={'index': 'id'}, inplace=True)\n",
    "\n",
    "# Reorder columns if needed\n",
    "desired_columns = [\n",
    "    'id', 'account_id', 'reputation', 'user_id', 'user_type',\n",
    "    'profile_image', 'display_name', 'link', 'creation_date',\n",
    "    'last_access_date'\n",
    "]\n",
    "df = df[desired_columns]\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "output_file = 'latest_stack_overflow_user_data.csv'\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f'Data saved to {output_file}')\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Your Stack Exchange API key\n",
    "api_key = \"YOUR_STACK_EXCHANGE_API_KEY\"  # Replace with your actual API key\n",
    "\n",
    "# Define the number of records and pagesize\n",
    "total_records = 10000\n",
    "pagesize = 100  # Max allowed pagesize by the API\n",
    "\n",
    "def fetch_user_data():\n",
    "    all_data = []\n",
    "    page = 1\n",
    "    while len(all_data) < total_records:\n",
    "        url = (\n",
    "            f\"https://api.stackexchange.com/2.3/users\"\n",
    "            f\"?order=desc&sort=creation&site=stackoverflow\"\n",
    "            f\"&pagesize={pagesize}&page={page}&key={api_key}\"\n",
    "        )\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            results = response.json().get('items', [])\n",
    "            if not results:\n",
    "                # No more results\n",
    "                break\n",
    "            for user in results:\n",
    "                data_row = {\n",
    "                    'account_id': user.get('account_id'),\n",
    "                    'reputation': user.get('reputation'),\n",
    "                    'user_id': user.get('user_id'),\n",
    "                    'user_type': user.get('user_type'),\n",
    "                    'profile_image': user.get('profile_image'),\n",
    "                    'display_name': user.get('display_name'),\n",
    "                    'link': user.get('link'),\n",
    "                    'creation_date': datetime.fromtimestamp(user.get('creation_date')),\n",
    "                    'last_access_date': datetime.fromtimestamp(user.get('last_access_date')),\n",
    "                }\n",
    "                all_data.append(data_row)\n",
    "                if len(all_data) >= total_records:\n",
    "                    break\n",
    "            page += 1\n",
    "        else:\n",
    "            print(f\"Error fetching data: {response.status_code}\")\n",
    "            break\n",
    "        # Add time.sleep to respect API rate limits\n",
    "        time.sleep(1.1)  # Sleep for 1.1 seconds\n",
    "    return all_data\n",
    "\n",
    "# Fetch data\n",
    "print(\"Fetching user data from Stack Exchange API...\")\n",
    "data = fetch_user_data()\n",
    "print(f\"Fetched {len(data)} records.\")\n",
    "\n",
    "# Create pandas DataFrame from data\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Add index column\n",
    "df.reset_index(inplace=True)\n",
    "df.rename(columns={'index': 'id'}, inplace=True)\n",
    "\n",
    "# Reorder columns if needed\n",
    "desired_columns = [\n",
    "    'id', 'account_id', 'reputation', 'user_id', 'user_type',\n",
    "    'profile_image', 'display_name', 'link', 'creation_date',\n",
    "    'last_access_date'\n",
    "]\n",
    "df = df[desired_columns]\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "output_file = 'latest_stack_overflow_user_data.csv'\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f'Data saved to {output_file}')\n",
    "\n",
    "\n",
    "# Verify that the API key is provided\n",
    "if api_key == \"YOUR_STACK_EXCHANGE_API_KEY\" or not api_key:\n",
    "    raise ValueError(\"Please replace 'YOUR_STACK_EXCHANGE_API_KEY' with your actual Stack Exchange API key.\")\n",
    "\n",
    "# Define the number of records and pagesize\n",
    "total_records = 25000\n",
    "pagesize = 50\n",
    "\n",
    "# Ensure pagesize does not exceed the maximum allowed by the API\n",
    "if pagesize > 100:\n",
    "    raise ValueError(\"The 'pagesize' parameter cannot exceed 100.\")\n",
    "\n",
    "total_pages = total_records // pagesize\n",
    "\n",
    "# List of URLs to fetch\n",
    "urls = []\n",
    "for i in range(1, total_pages + 1):\n",
    "    urls.append(\n",
    "        f\"https://api.stackexchange.com/2.3/questions\"\n",
    "        f\"?order=desc&sort=creation&site=stackoverflow\"\n",
    "        f\"&pagesize={pagesize}&page={i}&key={api_key}\"\n",
    "    )\n",
    "\n",
    "# Define the data fetching function\n",
    "def data_fetch(urls):\n",
    "    df_append = pd.DataFrame()\n",
    "    for url in urls:\n",
    "        time.sleep(1.1)  # Slightly over 1 second to comply with rate limits\n",
    "        raw_results = requests.get(url)\n",
    "        if raw_results.status_code == 200:\n",
    "            results = raw_results.json()\n",
    "            # Check if 'items' are in the results\n",
    "            if 'items' in results:\n",
    "                # Extract specific fields\n",
    "                owners = [item.get('owner', {}) for item in results['items']]\n",
    "                view_count = [item.get('view_count', 0) for item in results['items']]\n",
    "                score = [item.get('score', 0) for item in results['items']]\n",
    "\n",
    "                # Convert owner dictionaries to DataFrame\n",
    "                df_owners = pd.DataFrame(owners)\n",
    "\n",
    "                # Add 'view_count' and 'score' to the DataFrame\n",
    "                df_owners['view_count'] = view_count\n",
    "                df_owners['score'] = score\n",
    "\n",
    "                # Append to the main DataFrame\n",
    "                df_append = pd.concat([df_append, df_owners], ignore_index=True)\n",
    "            else:\n",
    "                print(f\"No 'items' found in response for URL: {url}\")\n",
    "        else:\n",
    "            print(f\"Error: {raw_results.status_code} on URL: {url}\")\n",
    "            print(f\"Response: {raw_results.text}\")\n",
    "            # Break the loop if there's a bad request to prevent further errors\n",
    "            if raw_results.status_code == 400:\n",
    "                print(\"Bad request encountered. Please check your API key and parameters.\")\n",
    "                break\n",
    "            # Handle rate limit exceeded\n",
    "            elif raw_results.status_code == 502:\n",
    "                print(\"Rate limit exceeded. Waiting for 60 seconds before retrying...\")\n",
    "                time.sleep(60)\n",
    "            else:\n",
    "                print(\"An unexpected error occurred.\")\n",
    "    return df_append\n",
    "\n",
    "# Fetch data and save to CSV\n",
    "df = data_fetch(urls)\n",
    "\n",
    "# Check if DataFrame is not empty before saving\n",
    "if not df.empty:\n",
    "    df.to_csv('latest_stack_overflow_data.csv', index=False)\n",
    "    print(\"Data saved to 'latest_stack_overflow_data.csv'\")\n",
    "else:\n",
    "    print(\"No data fetched. CSV file was not created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
